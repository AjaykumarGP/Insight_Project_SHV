{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "#NLP\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import guidedlda\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>top_1_score</th>\n",
       "      <th>top_2_score</th>\n",
       "      <th>Corrected_content</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>lang</th>\n",
       "      <th>average_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I understand sense we do these notes it hel...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>I understand sense we do these notes it helps ...</td>\n",
       "      <td>634</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Over the course of the six weeks, I was consta...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Over the course of the six weeks, I was consta...</td>\n",
       "      <td>566</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I feel like I made my best improvements in exp...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>I feel like I made my best improvements in exp...</td>\n",
       "      <td>407</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>*The knowledge checks and quizzes prepared me...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>*The knowledge checks and quizzes prepared me ...</td>\n",
       "      <td>403</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The study activity that I found the most helpf...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>The study activity that I found the most helpf...</td>\n",
       "      <td>397</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  top_1_score  \\\n",
       "0     I understand sense we do these notes it hel...            4   \n",
       "1  Over the course of the six weeks, I was consta...            4   \n",
       "2  I feel like I made my best improvements in exp...            3   \n",
       "3   *The knowledge checks and quizzes prepared me...            4   \n",
       "4  The study activity that I found the most helpf...            3   \n",
       "\n",
       "   top_2_score                                  Corrected_content  \\\n",
       "0            3  I understand sense we do these notes it helps ...   \n",
       "1            3  Over the course of the six weeks, I was consta...   \n",
       "2            4  I feel like I made my best improvements in exp...   \n",
       "3            3  *The knowledge checks and quizzes prepared me ...   \n",
       "4            4  The study activity that I found the most helpf...   \n",
       "\n",
       "   word_counts lang  average_score  \n",
       "0          634   en            3.5  \n",
       "1          566   en            3.5  \n",
       "2          407   en            3.5  \n",
       "3          403   en            3.5  \n",
       "4          397   en            3.5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('highest_cleaned').drop(['Unnamed: 0'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['mr', 'mrs', 'miss', 'ms', 'ahh', 'ah', 'want', 'feel', 'want', 'goal', 'ela', 'go', 'get', 'like','grade', 'use', 'make', \n",
    "                  'next', 'well', 'lea', 'also', 'thing', 'one', 'try', 'end', 'turn', 'work', 'math', 'try', 'sol', 'science','week', 'would',\n",
    "                 'class', 'need', 'exit', 'ticket', 'sure', 'strategy', 'exit','grade', 'good', 'best', 'able', 'lot', 'think', 'help',\n",
    "                'could', 'really', 'improve', 'time'])\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    '''\n",
    "    tags parts of speech to tokens\n",
    "    Expects a string and outputs the string and its part of speech\n",
    "    '''\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    '''\n",
    "    lemamtizes the tokens based on their part of speech\n",
    "    Expects a lits of tokens and outputs a list of lemmatized tokens\n",
    "    '''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = lemmatizer.lemmatize(text, get_wordnet_pos(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def reflection_tokenizer(text):\n",
    "    '''\n",
    "    Tokenizes a list of string, expects a list of strings and outputs a list of strings.\n",
    "    before tokenizing:\n",
    "    1)removes the non-alphanumeric charcaters like emojies\n",
    "    2)removes the numbers\n",
    "    3)lower cases the words\n",
    "    4)tokenizes the sentences\n",
    "    5)lemmatizes teh tokens\n",
    "    6)removes the tokens in stop words list\n",
    "     '''\n",
    "\n",
    "    text=re.sub(r'[\\W_]+', ' ', text) #keeps just alphnumeric characters\n",
    "    text=re.sub(r'\\d+', '', text) #removes numbers\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in word_tokenize(text)]\n",
    "    tokens = [word for word in tokens if len(word) >= 3]#removes smaller than 3 character\n",
    "    tokens = [word_lemmatizer(w) for w in tokens]\n",
    "    tokens = [s for s in tokens if s not in get_stopwords()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatize_token'] = df.Corrected_content.apply(reflection_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>top_1_score</th>\n",
       "      <th>top_2_score</th>\n",
       "      <th>Corrected_content</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>lang</th>\n",
       "      <th>average_score</th>\n",
       "      <th>lemmatize_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I understand sense we do these notes it hel...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>I understand sense we do these notes it helps ...</td>\n",
       "      <td>634</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[understand, sense, note, understand, detailed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Over the course of the six weeks, I was consta...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Over the course of the six weeks, I was consta...</td>\n",
       "      <td>566</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[course, six, constantly, pretty, similar, stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I feel like I made my best improvements in exp...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>I feel like I made my best improvements in exp...</td>\n",
       "      <td>407</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[improvement, expand, idea, write, regularly, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>*The knowledge checks and quizzes prepared me...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>*The knowledge checks and quizzes prepared me ...</td>\n",
       "      <td>403</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[knowledge, check, quiz, prepared, exam, learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The study activity that I found the most helpf...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>The study activity that I found the most helpf...</td>\n",
       "      <td>397</td>\n",
       "      <td>en</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[study, activity, found, helpful, review, know...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  top_1_score  \\\n",
       "0     I understand sense we do these notes it hel...            4   \n",
       "1  Over the course of the six weeks, I was consta...            4   \n",
       "2  I feel like I made my best improvements in exp...            3   \n",
       "3   *The knowledge checks and quizzes prepared me...            4   \n",
       "4  The study activity that I found the most helpf...            3   \n",
       "\n",
       "   top_2_score                                  Corrected_content  \\\n",
       "0            3  I understand sense we do these notes it helps ...   \n",
       "1            3  Over the course of the six weeks, I was consta...   \n",
       "2            4  I feel like I made my best improvements in exp...   \n",
       "3            3  *The knowledge checks and quizzes prepared me ...   \n",
       "4            4  The study activity that I found the most helpf...   \n",
       "\n",
       "   word_counts lang  average_score  \\\n",
       "0          634   en            3.5   \n",
       "1          566   en            3.5   \n",
       "2          407   en            3.5   \n",
       "3          403   en            3.5   \n",
       "4          397   en            3.5   \n",
       "\n",
       "                                     lemmatize_token  \n",
       "0  [understand, sense, note, understand, detailed...  \n",
       "1  [course, six, constantly, pretty, similar, stu...  \n",
       "2  [improvement, expand, idea, write, regularly, ...  \n",
       "3  [knowledge, check, quiz, prepared, exam, learn...  \n",
       "4  [study, activity, found, helpful, review, know...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts, matrix of documents and tokens\n",
    "token_vectorizer = CountVectorizer(tokenizer = reflection_tokenizer, min_df=10, stop_words=get_stopwords(), ngram_range=(1, 4))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahrzad/anaconda3/envs/insight/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#function maps the column of the dataframe to a matrix of documents in the rows and token counts as columns,\n",
    "# this is bag of words representation of the documents\n",
    "X_ngrams = token_vectorizer.fit_transform(df.Corrected_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23150, 7317)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of terms (words or n_grams of words)\n",
    "tf_feature_names = token_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of words frequency\n",
    "word2id = dict((v, idx) for idx, v in enumerate(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywrods for seeded topics that I want GuidedLDA model to converge to \n",
    "seed_topic_list_6 = [['take', 'note', 'compare', 'classmate', 'highlight', 'underline', 'jot', 'write', 'topic', 'main', 'complete', 'point', 'copy', 'slide'],\n",
    "                   ['read', 'study','review', 'skim', 'textbook', 'compare', 'note','connect', 'sketch', 'summarize', 'relationship', 'map', 'concept', 'diagram', 'chart'],\n",
    "                   ['question', 'essay','assignment', 'exam', 'test', 'quiz', 'answer', 'practice', 'review', 'repeat', 'strength', 'weak', 'solve', 'problem', 'identify'],\n",
    "                   ['plan', 'calendar', 'task', 'list', 'manage', 'procrastinate', 'due','stress', 'manage', 'anxiety', 'express', 'break', 'sleep', 'nap', 'eat', 'exercise'],\n",
    "                   ['group','partner', 'classmate', 'brainstorm', 'ask', 'answer', 'verify', 'peer', 'teach', 'clarify'],\n",
    "                   ['ask','aid', 'resource', 'teacher', 'tutor', 'peer', 'verify', 'explain', 'clear', 'talk']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 23150\n",
      "INFO:guidedlda:vocab_size: 7317\n",
      "INFO:guidedlda:n_words: 494675\n",
      "INFO:guidedlda:n_topics: 6\n",
      "INFO:guidedlda:n_iter: 100\n",
      "/Users/shahrzad/anaconda3/envs/insight/lib/python3.6/site-packages/guidedlda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -5252196\n",
      "INFO:guidedlda:<10> log likelihood: -3942594\n",
      "INFO:guidedlda:<20> log likelihood: -3800728\n",
      "INFO:guidedlda:<30> log likelihood: -3749330\n",
      "INFO:guidedlda:<40> log likelihood: -3721371\n",
      "INFO:guidedlda:<50> log likelihood: -3702690\n",
      "INFO:guidedlda:<60> log likelihood: -3689292\n",
      "INFO:guidedlda:<70> log likelihood: -3678675\n",
      "INFO:guidedlda:<80> log likelihood: -3671903\n",
      "INFO:guidedlda:<90> log likelihood: -3666396\n",
      "INFO:guidedlda:<99> log likelihood: -3663414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x1352556a0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate the guidedlda as model with parameters like number of topics \n",
    "model = guidedlda.GuidedLDA(n_topics=6, n_iter=100, random_state=7, refresh=10)\n",
    "#seed_topics is the dictionary {word_id to topic_id}\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list_6):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "#build the model on the dataset with 6 topics\n",
    "#tseed_confidence: how much extra boost should be given to a term between 0 to 1\n",
    "model.fit(X_ngrams, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guided_LDA_6topics.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "joblib.dump(model, \"Guided_LDA_6topics.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: write book read reading idea story essay paragraph word people main add point evidence start\n",
      "Topic 1: read study note test question take quiz reading answer look understand book word know score\n",
      "Topic 2: question quiz study check understand note answer problem knowledge ask mistake practice take knowledge check look\n",
      "Topic 3: finish homework assignment complete day school reflection study start project something keep last home win\n",
      "Topic 4: answer question understand learn find know problem way lab look write easy see number put\n",
      "Topic 5: ask finish teacher talk school attention stay know focus pay extra pay attention homework keep test\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 15\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "     topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "     print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dics = {topic 3:'finish homework and complete assignment',\n",
    "topic 2:'check past quizzes and questions and understand answers',\n",
    "topic 5:'talking and asking teacher and pay attention',\n",
    "topic 1:'read, study notes and books',\n",
    "topic 4:'answering questions and understand and learn the problems',\n",
    "topic 0:'write story, essay and books'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahrzad/anaconda3/envs/insight/lib/python3.6/site-packages/guidedlda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.10817190e-03 3.62491317e-02 6.12804332e-01 6.97971179e-03\n",
      "  2.98556991e-01 4.13016617e-02]\n",
      " [3.19106859e-04 7.85588105e-02 7.02524670e-01 1.56446752e-02\n",
      "  2.02307714e-01 6.45023218e-04]\n",
      " [9.66845930e-01 7.31613319e-05 4.14169033e-05 3.28841281e-02\n",
      "  4.11003918e-05 1.14263566e-04]\n",
      " ...\n",
      " [6.45386207e-01 7.97752399e-03 1.73387809e-02 2.28949904e-03\n",
      "  2.63954147e-03 3.24368448e-01]\n",
      " [9.37838023e-01 5.85436764e-02 2.31894839e-04 1.56189903e-03\n",
      "  1.59037317e-03 2.34133890e-04]\n",
      " [3.97658885e-03 7.73764124e-01 2.20293856e-01 3.62350147e-04\n",
      "  7.01562871e-04 9.01518172e-04]]\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(X_ngrams)\n",
    "print(doc_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.19106859e-04, 7.85588105e-02, 7.02524670e-01, 1.56446752e-02,\n",
       "       2.02307714e-01, 6.45023218e-04])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic 0  topic 1  topic 2  topic 3  topic 4  topic 5\n",
       "0     0.00     0.04     0.61     0.01     0.30     0.04\n",
       "1     0.00     0.08     0.70     0.02     0.20     0.00\n",
       "2     0.97     0.00     0.00     0.03     0.00     0.00\n",
       "3     0.05     0.09     0.79     0.00     0.04     0.02\n",
       "4     0.00     0.01     0.91     0.00     0.08     0.00\n",
       "5     0.00     0.06     0.92     0.00     0.02     0.00\n",
       "6     0.25     0.00     0.00     0.00     0.59     0.16\n",
       "7     0.06     0.00     0.54     0.16     0.24     0.00\n",
       "8     0.28     0.00     0.00     0.72     0.00     0.00\n",
       "9     0.00     0.00     0.13     0.83     0.02     0.03"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_label = ['topic {}'.format(i) for i in range(6)]  # number of topics\n",
    "topic_vector = pd.DataFrame(doc_topic, columns = columns_label)#dataframe of doc-topics\n",
    "topic_vector.round(2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_threshold(doc_topic, topic_vector):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the topic number if the probablity of a topic being in a document is more than value\n",
    "    \"\"\"\n",
    "    \n",
    "    topic_num_list = []\n",
    "    for i in range(len(topic_vector)):\n",
    "        topic_num = [idx for idx, value in enumerate(doc_topic[i]) if value >= 0.25]\n",
    "        if topic_num != []:\n",
    "            topic_num = topic_num\n",
    "        else:\n",
    "            topic_num = 'None'\n",
    "        topic_num_list.append(topic_num)\n",
    "    return topic_num_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topic=topic_threshold(doc_topic, topic_vector)\n",
    "#print(num_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_topic = pd.DataFrame({'topics': num_topic, 'reflection': df.Corrected_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_doc_topic.loc[:50]\n",
    "#df_doc_topic.reflection[43]\n",
    "#df_doc_topic.topics[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahrzad/anaconda3/envs/insight/lib/python3.6/site-packages/guidedlda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "/Users/shahrzad/anaconda3/envs/insight/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el69151082186407822958678\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el69151082186407822958678_data = {\"mdsDat\": {\"x\": [-158.1439971923828, 279.71685791015625, 151.26080322265625, 88.5022201538086, 48.39250946044922, -119.35102081298828], \"y\": [-155.4862060546875, -79.20439910888672, 163.59097290039062, -276.4104309082031, -46.21366882324219, 116.45524597167969], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [21.206581837100206, 18.57210538543996, 17.10821625909861, 16.666463309089433, 14.90153949264498, 11.5450937166268]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [3730.0, 3984.0, 6029.0, 5694.0, 3859.0, 4289.0, 3635.0, 3674.0, 4484.0, 2609.0, 1704.0, 2378.0, 873.0, 2641.0, 1991.0, 1087.0, 1530.0, 1704.0, 4005.0, 1850.0, 1137.0, 2522.0, 948.0, 3574.0, 1259.0, 2821.0, 920.0, 616.0, 646.0, 1167.0, 217.35379293347927, 191.84398201691675, 162.25260135370422, 148.98749967709173, 134.7020055638167, 115.31454926722922, 113.27376439390422, 112.2533719572417, 111.2329795205792, 102.0494475906167, 100.0086627172917, 98.98827028062921, 91.84552322399169, 91.84552322399169, 82.66199129402919, 74.4988518007292, 69.39688961741668, 69.39688961741668, 62.254142560779165, 60.21335768745417, 59.192965250791666, 59.192965250791666, 54.09100306747916, 50.00943332082916, 48.989040884166656, 47.96864844750415, 43.88707870085415, 43.88707870085415, 43.88707870085415, 42.86668626419166, 480.6150415924045, 608.1640961752171, 204.08869125686675, 112.2533719572417, 863.2622053408422, 76.5396366740542, 153.06906942374172, 579.593107948667, 1669.3722303042177, 309.1891122331043, 557.144474342092, 257.1490979633168, 283.67930131654185, 1712.228712644043, 2432.6257729277686, 1664.2702681209055, 190.82358958025424, 304.08715004979183, 448.98287605586694, 977.5461582470424, 284.6996937532043, 198.98672907355424, 997.9540069802923, 675.5099969949421, 386.7389374194544, 198.98672907355424, 517.3491693122545, 617.3476281051795, 696.9382381648546, 623.4699827251545, 288.78126349985433, 658.1633255716796, 653.061363388367, 429.5954197592794, 841.8339641709297, 501.0228903256545, 433.67698950592944, 423.4730651393045, 607.1437037385546, 432.656597069267, 591.8378171886171, 426.5342424492919, 533.6754482988545, 872.9678937621495, 170.36642029838487, 148.22966154541695, 139.5674515986034, 106.84354735508563, 90.48159523332671, 88.55665968959038, 86.63172414585404, 84.7067886021177, 81.8193852865132, 80.85691751464502, 73.15717533969966, 72.19470756783149, 72.19470756783149, 72.19470756783149, 67.38236870849063, 66.41990093662245, 58.72015876167709, 57.757690989808914, 56.79522321794075, 54.870287674204405, 52.945352130468066, 51.02041658673173, 50.05794881486355, 47.17054549925904, 46.208077727390865, 46.208077727390865, 41.39573886805002, 41.39573886805002, 40.43327109618185, 40.43327109618185, 91.44406300519489, 124.16796724871267, 166.5165492109122, 966.327267633362, 111.65588621442647, 137.6425160548671, 463.9190907181769, 920.1288145836897, 530.3293669770807, 231.96435769794778, 200.20292122629817, 117.4306928456355, 198.27798568256182, 485.0933816992767, 495.6805271898265, 1515.8963653700873, 179.99109801706658, 523.5920925740035, 478.3561072961995, 1061.611577048311, 485.0933816992767, 1857.572424383288, 2387.8921666826504, 906.6542657775353, 583.26509442983, 1380.1884095366752, 476.4311717524632, 1846.0228111208698, 512.0424793115855, 1218.4938238628226, 1242.5555181595269, 341.68568369091923, 464.88155849004505, 955.7401221428121, 393.65894337180043, 884.5175070245674, 733.4100668412647, 675.6620005291744, 515.8923503990582, 670.8496616698336, 596.7396432359844, 232.32764806868943, 212.55592362558582, 192.78419918248215, 167.0809574064474, 127.5375085202401, 127.5375085202401, 124.57174985377455, 102.82285296636054, 96.89133563342943, 86.01688718972244, 84.03971474541208, 83.05112852325688, 78.10819741248098, 72.17668007954988, 68.22233519092916, 68.22233519092916, 64.26799030230842, 64.26799030230842, 63.279404080153235, 61.302231635842865, 55.37071430291178, 52.404955636446225, 52.404955636446225, 50.42778319213586, 48.4506107478255, 48.4506107478255, 47.462024525670316, 47.462024525670316, 47.462024525670316, 46.473438303515124, 230.35047562437907, 463.6568240530022, 245.1792689567068, 572.4013084900722, 415.2160991673982, 278.791200509983, 145.33206051903338, 232.32764806868943, 211.5673374034306, 207.6129925148099, 1164.5644555610265, 539.7779631589511, 194.7613716267925, 1230.7997324454238, 556.5839289355894, 649.5110338181765, 431.0334787218811, 745.4038973672292, 734.5294489235222, 2080.983883498881, 780.9930013648158, 500.2345142727439, 769.1299666989536, 443.88509960989853, 329.2090978398973, 263.9624071776553, 312.4031320632592, 402.3644782793808, 877.8744511360237, 1345.475734215425, 751.3354147001603, 623.8077920421417, 751.3354147001603, 432.0220649440363, 408.2959956123119, 630.727895597228, 431.0334787218811, 544.7208942697272, 499.2459280505887, 576.355653378693, 463.6568240530022, 538.789376936796, 515.0633076050717, 436.9649960548123, 129.50611053976004, 68.86917282175479, 67.84142811466995, 63.730449286330604, 63.730449286330604, 54.48074692256709, 51.39751280131259, 51.39751280131259, 50.36976809422775, 49.34202338714291, 47.286533972973245, 43.17555514463391, 40.092321023379405, 38.036831609209734, 38.036831609209734, 35.981342195040064, 33.92585278087039, 33.92585278087039, 33.92585278087039, 31.87036336670073, 31.87036336670073, 31.87036336670073, 30.842618659615894, 29.81487395253106, 28.787129245446224, 28.787129245446224, 27.759384538361388, 27.759384538361388, 27.759384538361388, 26.731639831276556, 26.731639831276556, 172.6713882373231, 200.4204953286136, 136.70032348935385, 79.14661989260313, 479.96705565568874, 51.39751280131259, 461.4676509281617, 254.89096480410987, 271.3348801174672, 255.91870951119472, 2756.421581848598, 171.64364353023825, 338.13828607798143, 56.53623633673676, 480.9948003627735, 139.78355761060837, 65.78593870050028, 2204.522674144042, 2748.199624191919, 463.5231403423313, 166.5049199948141, 1287.7743954243688, 1816.0351748659739, 161.3661964593899, 1575.5429134081228, 148.00551526728705, 227.14185771281933, 1072.9757516436382, 1316.551247222744, 1738.9543218346114, 704.0154018001826, 617.6848464050565, 1170.6114988166976, 289.8342848449942, 594.0467181421053, 549.8536957374574, 365.88739316927206, 593.0189734350206, 245.64126244034634, 757.4581265685941, 607.4073993342082, 538.5485039595243, 452.2179485643981, 284.69556130957005, 345.3324990275753, 404.9416920384957, 294.97300838041843, 294.97300838041843, 100.11732424097134, 76.09156899866244, 70.08513018808523, 61.07547197221938, 58.07225256693077, 51.06474062125735, 50.063667486161144, 49.062594351064945, 48.06152121596873, 46.059374945776334, 46.059374945776334, 38.0507898650067, 35.04757045971809, 35.04757045971809, 35.04757045971809, 35.04757045971809, 35.04757045971809, 31.04327791933328, 31.04327791933328, 31.04327791933328, 30.042204784237075, 28.040058514044667, 28.040058514044667, 28.040058514044667, 28.040058514044667, 26.03791224385226, 26.03791224385226, 26.03791224385226, 26.03791224385226, 25.036839108756055, 252.2804407755943, 92.10873916020171, 120.13878694289542, 125.14415261837642, 471.51545736166287, 56.070106296738366, 100.11732424097134, 337.3716572587716, 232.25897807367022, 283.31370796357663, 108.12590932174096, 50.063667486161144, 58.07225256693077, 255.28366018088292, 636.6925246525365, 342.3770229342526, 144.16454218520428, 129.14844515876123, 76.09156899866244, 1105.1947518775598, 765.8309590799468, 525.5734066568579, 332.3662915832906, 260.28902585636393, 274.3040497477108, 552.6023813044553, 816.8856889698532, 311.3437557462703, 205.2300034260727, 357.3931199606957, 268.2976109371335, 918.9951487496659, 614.66891568042, 345.38024233954127, 469.5133110914705, 337.3716572587716, 261.2900989914601, 246.27400196501708, 313.34590201646273, 236.26327061405502, 272.30190347751835, 399.43819163473626, 316.3491214217513, 287.3180005039614, 289.32014677415384, 322.3555602323285, 286.3169273688652, 276.3061960179032, 280.310488558288, 264.70997664079664, 157.23778583140586, 153.25733431994695, 149.276882808488, 111.46259344962833, 85.58965862514536, 83.5994328694159, 75.63852984649807, 72.65319121290388, 70.66296545717444, 70.66296545717444, 69.6678525793097, 67.67762682358024, 65.68740106785077, 62.70206243425658, 62.70206243425658, 61.706949556391855, 58.72161092279767, 58.72161092279767, 57.726498044932946, 56.73138516706821, 56.73138516706821, 55.736272289203484, 52.75093365560929, 52.75093365560929, 52.75093365560929, 49.765595022015106, 49.765595022015106, 45.78514351055619, 44.790030632691455, 44.790030632691455, 118.42838359468143, 155.2475600756764, 238.83704181631364, 153.25733431994695, 129.37462525119344, 116.43815783895197, 184.10583353375353, 91.56033589233374, 201.02275245745395, 2277.8233285611436, 222.91523577047798, 85.58965862514536, 136.34041539624653, 341.33366823638073, 121.4137222282756, 228.88591303766637, 490.6005999160901, 142.31109266343492, 354.2701356486222, 477.66413250384863, 142.31109266343492, 743.3592708937313, 285.60734707595594, 225.90057440407213, 141.3159797855702, 392.08442500748197, 305.5096046332505, 708.5303201684659, 321.4314106790862, 270.680653907985, 346.3092326257044, 713.5058845577895, 316.4558462897625, 347.3043455035691, 198.03741382385977, 227.89080015980164, 286.60245995382064, 290.58291146527955, 268.6904281522555, 271.67576678584976, 226.8956872819369, 256.74907361787876, 270.680653907985, 233.86147742698998, 231.87125167126055], \"Term\": [\"write\", \"read\", \"study\", \"question\", \"quiz\", \"finish\", \"ask\", \"answer\", \"note\", \"reading\", \"book\", \"assignment\", \"knowledge check\", \"homework\", \"school\", \"knowledge\", \"talk\", \"teacher\", \"test\", \"problem\", \"mistake\", \"check\", \"reflection\", \"understand\", \"stay\", \"complete\", \"story\", \"win\", \"credit\", \"attention\", \"reflection accomplish\", \"tam\", \"reflection finish\", \"reflection change\", \"preview\", \"win outside\", \"accomplish step\", \"stroke\", \"preview note\", \"due date\", \"reflection accomplish step\", \"small win outside\", \"lordly wise\", \"lordly\", \"reflection complete\", \"wordy wise\", \"stay top\", \"outside classroom\", \"win outside classroom\", \"assignment due\", \"assignment none\", \"tam lesson\", \"small win outside classroom\", \"object stroke\", \"change finish\", \"block reflection\", \"color apply\", \"procrastination\", \"block finish\", \"reflection change yes\", \"small win\", \"win\", \"wise\", \"upcoming assignment\", \"reflection\", \"wordy\", \"block\", \"due\", \"assignment\", \"sleep\", \"small\", \"early\", \"weekend\", \"homework\", \"finish\", \"complete\", \"college\", \"manage\", \"accomplish\", \"school\", \"outside\", \"upcoming\", \"day\", \"project\", \"plan\", \"finish homework\", \"english\", \"home\", \"start\", \"last\", \"stress\", \"something\", \"keep\", \"pretty\", \"study\", \"focus\", \"still\", \"much\", \"quiz\", \"everything\", \"test\", \"hard\", \"take\", \"knowledge check\", \"independent practice\", \"study activity\", \"predict topic\", \"polynomial\", \"quadratic\", \"schedule realistic\", \"study schedule realistic\", \"activity helpful\", \"enough answer\", \"little mistake\", \"study activity helpful\", \"take knowledge\", \"take knowledge check\", \"notation\", \"enough answer question\", \"square root\", \"continue ask\", \"understood instruction\", \"topic knowledge\", \"topic knowledge check\", \"note classwork\", \"helpful study\", \"function notation\", \"future continue\", \"understood question\", \"topic assessment\", \"quiz exam\", \"predict topic assessment\", \"helpful ask\", \"predict topic knowledge\", \"collins\", \"silly mistake\", \"study schedule\", \"knowledge\", \"concept quiz\", \"silly\", \"exam\", \"mistake\", \"concept\", \"predict\", \"function\", \"confident answer\", \"independent\", \"classwork\", \"understood\", \"check\", \"factor\", \"helpful\", \"confident\", \"problem\", \"solve\", \"quiz\", \"question\", \"practice\", \"wrong\", \"understand\", \"felt\", \"study\", \"topic\", \"answer\", \"note\", \"future\", \"continue\", \"ask\", \"confuse\", \"take\", \"look\", \"learn\", \"first\", \"test\", \"know\", \"raise hand\", \"pillar\", \"enrichment\", \"ask extra\", \"weekly report\", \"ask extra credit\", \"crosby\", \"pillar point\", \"unfinished\", \"teacher talk\", \"stop talk\", \"earn pillar\", \"know reach\", \"stay school finish\", \"cabrera\", \"credit packet\", \"extra credit packet\", \"receive deduction\", \"extra packet\", \"know achieve\", \"sketch day\", \"stuck back\", \"stay lunch\", \"teacher ask\", \"talk talk\", \"stuck back note\", \"classmate adult\", \"tardy\", \"current increase\", \"talk much\", \"deduction\", \"extra credit\", \"stay school\", \"credit\", \"language art\", \"ask teacher\", \"folder\", \"cod\", \"english language\", \"english language art\", \"talk\", \"raise\", \"report\", \"teacher\", \"art\", \"pay attention\", \"language\", \"pay\", \"extra\", \"ask\", \"attention\", \"packet\", \"stay\", \"reach\", \"hand\", \"participate\", \"computer\", \"bring\", \"school\", \"finish\", \"focus\", \"keep\", \"know\", \"english\", \"high\", \"homework\", \"home\", \"complete\", \"assignment\", \"test\", \"something\", \"question\", \"study\", \"understand\", \"question read\", \"chapter note\", \"study quiz let\", \"quiz let study\", \"let study\", \"reading passage\", \"landforms\", \"first read\", \"read question read\", \"passage read\", \"refer note\", \"deposition\", \"read question first\", \"workbook page\", \"note put\", \"passage answer\", \"note reading\", \"learn target assessment\", \"target assessment\", \"study session\", \"take note study\", \"night test\", \"carefully look\", \"reading question carefully\", \"study learn target\", \"erosion deposition\", \"put away write\", \"question read passage\", \"everything remember\", \"look learn target\", \"reread story\", \"read passage\", \"question carefully\", \"read question carefully\", \"day test\", \"passage\", \"workbook\", \"read question\", \"learn target\", \"target\", \"quiz let\", \"read\", \"study note\", \"chapter\", \"study day\", \"carefully\", \"test study\", \"take read\", \"note\", \"study\", \"take note\", \"consummative\", \"reading\", \"test\", \"note take\", \"take\", \"definition\", \"study test\", \"look\", \"quiz\", \"question\", \"book\", \"word\", \"answer\", \"let\", \"score\", \"review\", \"text\", \"day\", \"reread\", \"understand\", \"know\", \"learn\", \"back\", \"night\", \"every\", \"write\", \"answer question\", \"story\", \"tape\", \"tape diagram\", \"volcano\", \"ratio\", \"mass\", \"technology\", \"precise\", \"double number\", \"double number line\", \"pun\", \"atom\", \"found effective\", \"nett square\", \"ratio rate\", \"pun nett\", \"pun nett square\", \"nett\", \"periodic table\", \"sba test\", \"periodic\", \"roller\", \"roller coaster\", \"coaster\", \"linear equation\", \"effective test\", \"earthquake\", \"eruption\", \"path\", \"specific heat\", \"data table\", \"data\", \"rate\", \"calculation\", \"number line\", \"lab\", \"interpret\", \"label\", \"line\", \"table\", \"graph\", \"experiment\", \"sba\", \"heat\", \"error\", \"find\", \"number\", \"chose\", \"diagram\", \"element\", \"answer\", \"learn\", \"way\", \"different\", \"equation\", \"evidence\", \"problem\", \"understand\", \"show\", \"difficult\", \"easy\", \"information\", \"question\", \"know\", \"see\", \"look\", \"put\", \"part\", \"confuse\", \"right\", \"solve\", \"wrong\", \"write\", \"practice\", \"word\", \"give\", \"test\", \"check\", \"hard\", \"take\", \"dish\", \"score point\", \"anne\", \"side dish\", \"main dish\", \"risk\", \"craft\", \"body paragraph\", \"take risk\", \"grammar punctuation\", \"author craft\", \"writer\", \"heading\", \"point first\", \"score point first\", \"point first semester\", \"score point first semester\", \"write write\", \"third semester\", \"ink\", \"increase score point\", \"red ink\", \"pod cast\", \"point third semester\", \"hook\", \"point third\", \"score point third\", \"score point third semester\", \"highlight key\", \"art score\", \"vocab com\", \"write reading\", \"reader\", \"grammar\", \"punctuation\", \"introduction\", \"book club\", \"transition\", \"reading write\", \"spell\", \"write\", \"character\", \"narrative\", \"main idea\", \"main\", \"club\", \"sentence\", \"idea\", \"claim\", \"paragraph\", \"story\", \"criterion\", \"book\", \"notebook\", \"side\", \"author\", \"essay\", \"evidence\", \"reading\", \"add\", \"detail\", \"people\", \"read\", \"point\", \"word\", \"semester\", \"proud\", \"give\", \"start\", \"put\", \"first\", \"thought\", \"focus\", \"look\", \"score\", \"take\"], \"Total\": [3730.0, 3984.0, 6029.0, 5694.0, 3859.0, 4289.0, 3635.0, 3674.0, 4484.0, 2609.0, 1704.0, 2378.0, 873.0, 2641.0, 1991.0, 1087.0, 1530.0, 1704.0, 4005.0, 1850.0, 1137.0, 2522.0, 948.0, 3574.0, 1259.0, 2821.0, 920.0, 616.0, 646.0, 1167.0, 217.40354278061997, 191.89373186405746, 162.30235120084492, 149.03724952423244, 134.7517554109574, 115.36429911436991, 113.3235142410449, 112.30312180438239, 111.2827293677199, 102.09919743775738, 100.05841256443239, 99.0380201277699, 91.89527307113238, 91.89527307113238, 82.71174114116988, 74.54860164786989, 69.44663946455736, 69.44663946455736, 62.303892407919854, 60.26310753459486, 59.242715097932354, 59.242715097932354, 54.140752914619846, 50.05918316796985, 49.038790731307344, 48.01839829464484, 43.936828547994836, 43.936828547994836, 43.936828547994836, 42.916436111332345, 485.67015711502626, 616.2224311031274, 210.9529262335407, 115.26888047084793, 948.2277288673077, 78.5915327913873, 166.0762838922106, 701.2634044380043, 2378.4529343055774, 366.5112937081191, 711.5642490244708, 304.1049238532097, 341.1431385942499, 2641.515564661986, 4289.898674316471, 2821.087096542594, 229.0684520644678, 406.7275465719364, 678.1745416538059, 1991.9780433778644, 394.0227401965543, 248.10204181906673, 2146.558978704082, 1280.9833247470822, 651.339152092325, 256.4005864285337, 1027.6978886987986, 1386.8992008116404, 1678.954805407917, 1591.704215634431, 461.2249645780501, 1894.950455153556, 1908.0704050750703, 1219.1990212154076, 6029.223373486424, 1966.874500235044, 1383.639211841546, 1365.6950551185905, 3859.895011134838, 1453.170746849846, 4005.306979044437, 1539.2840948278297, 3861.8185347991803, 873.0182228559381, 170.41674939217353, 148.2799906392056, 139.61778069239207, 106.89387644887427, 90.53192432711535, 88.60698878337901, 86.68205323964267, 84.75711769590633, 81.86971438030183, 80.90724660843365, 73.2075044334883, 72.24503666162012, 72.24503666162012, 72.24503666162012, 67.43269780227926, 66.47023003041109, 58.770487855465724, 57.80802008359755, 56.845552311729385, 54.92061676799304, 52.9956812242567, 51.07074568052036, 50.108277908652184, 47.220874593047675, 46.2584068211795, 46.2584068211795, 41.44606796183865, 41.44606796183865, 40.48360018997048, 40.48360018997048, 93.49653836917594, 128.32927517084065, 173.70962536133834, 1087.961388883607, 115.81719413655445, 144.88705809824958, 526.0724915843512, 1137.2688955635958, 642.2313931107128, 265.05010024991117, 228.3992139121783, 126.54402329926724, 228.37205267509998, 626.7084997520752, 652.8433567200169, 2522.5450399263773, 210.07362116374134, 757.1664494825848, 688.8426854400365, 1850.7334832086292, 721.3969706757694, 3859.895011134838, 5694.423799916856, 1802.0595116294148, 1097.832391528539, 3574.5491664230913, 892.3669189963122, 6029.223373486424, 1011.7983553080771, 3674.5008208216523, 4484.824302952751, 605.5799986901053, 1020.7488742961988, 3635.8704710235697, 788.2814434269183, 3861.8185347991803, 2778.1757182540327, 2415.550628280045, 1644.3262294366086, 4005.306979044437, 3150.391122389077, 232.3777159779752, 212.6059915348716, 192.83426709176794, 167.13102531573318, 127.58757642952587, 127.58757642952587, 124.62181776306032, 102.87292087564632, 96.94140354271521, 86.06695509900821, 84.08978265469786, 83.10119643254265, 78.15826532176675, 72.22674798883565, 68.27240310021493, 68.27240310021493, 64.31805821159419, 64.31805821159419, 63.32947198943899, 61.35229954512862, 55.42078221219754, 52.45502354573198, 52.45502354573198, 50.47785110142162, 48.50067865711126, 48.50067865711126, 47.512092434956074, 47.512092434956074, 47.512092434956074, 46.52350621280088, 233.46172084365236, 493.2982726255005, 260.5352234159301, 646.5149996412621, 467.0537585262702, 307.90832839145406, 155.58605279494415, 260.65891468103075, 235.50011438146987, 231.54576949284916, 1530.9515505319575, 676.4447017716143, 218.04863396052025, 1704.895876415404, 710.4166058681237, 897.909610802333, 561.5446580043814, 1064.7676926250833, 1063.923050904898, 3635.8704710235697, 1167.8597288967037, 723.0580131972808, 1259.3983108854263, 670.6777865397688, 464.53610062286475, 347.0216322450365, 439.84230160749775, 636.9852120516402, 1991.9780433778644, 4289.898674316471, 1966.874500235044, 1908.0704050750703, 3150.391122389077, 1027.6978886987986, 931.2549490773565, 2641.515564661986, 1386.8992008116404, 2821.087096542594, 2378.4529343055774, 4005.306979044437, 1894.950455153556, 5694.423799916856, 6029.223373486424, 3574.5491664230913, 129.55578686419653, 68.91884914619126, 67.89110443910643, 63.780125610767065, 63.780125610767065, 54.530423247003554, 51.44718912574905, 51.44718912574905, 50.41944441866421, 49.39169971157937, 47.33621029740971, 43.22523146907037, 40.14199734781587, 38.086507933646196, 38.086507933646196, 36.031018519476525, 33.975529105306855, 33.975529105306855, 33.975529105306855, 31.920039691137198, 31.920039691137198, 31.920039691137198, 30.892294984052363, 29.864550276967528, 28.836805569882692, 28.836805569882692, 27.809060862797857, 27.809060862797857, 27.809060862797857, 26.781316155713025, 26.781316155713025, 176.7015160732185, 211.17313323328406, 142.5634118082274, 81.17346866134996, 523.8732216929509, 52.442302003613776, 526.5844147250187, 282.58463527895975, 302.0703753608337, 292.83178449059335, 3984.9919399779756, 196.71748192324716, 423.6067824612464, 59.47331597677774, 627.7242812577734, 172.0001430602548, 71.84205383551398, 4484.824302952751, 6029.223373486424, 725.0868451148257, 214.91228340085996, 2609.2814037738826, 4005.306979044437, 216.43375650167513, 3861.8185347991803, 195.98821967782456, 355.8995803510829, 2778.1757182540327, 3859.895011134838, 5694.423799916856, 1704.230994013829, 1461.4575127639082, 3674.5008208216523, 507.4936693577064, 1553.2239739598094, 1481.3645355944482, 818.2460123000096, 2146.558978704082, 423.50406691504463, 3574.5491664230913, 3150.391122389077, 2415.550628280045, 1800.675639475248, 637.3010845484, 1280.0369459215158, 3730.4406614811232, 789.1308745330512, 920.0779893571485, 100.1672672811277, 76.1415120388188, 70.1350732282416, 61.12541501237573, 58.12219560708712, 51.1146836614137, 50.113610526317494, 49.112537391221295, 48.11146425612508, 46.109317985932684, 46.109317985932684, 38.10073290516305, 35.09751349987444, 35.09751349987444, 35.09751349987444, 35.09751349987444, 35.09751349987444, 31.093220959489635, 31.093220959489635, 31.093220959489635, 30.09214782439343, 28.09000155420102, 28.09000155420102, 28.09000155420102, 28.09000155420102, 26.087855284008615, 26.087855284008615, 26.087855284008615, 26.087855284008615, 25.08678214891241, 275.59283587056956, 97.0689563776886, 130.77587547360167, 136.74370892095084, 580.2617542445753, 59.20328345814922, 114.05147137083978, 477.1697500399451, 318.0159178413434, 421.02918748687796, 136.93618470467428, 55.25233406174167, 66.27888960532559, 408.8072507340623, 1376.8585904399536, 632.248044879626, 211.48068058845058, 187.01279077240284, 97.26730527851883, 3674.5008208216523, 2415.550628280045, 1483.1067606157346, 830.5419493879123, 586.1752912696953, 635.9097033628491, 1850.7334832086292, 3574.5491664230913, 829.603112202567, 440.4114105591888, 1052.1143763860543, 702.5385033821825, 5694.423799916856, 3150.391122389077, 1284.3237423732921, 2778.1757182540327, 1402.8590688079423, 898.1573132023234, 788.2814434269183, 1423.9486251825458, 721.3969706757694, 1097.832391528539, 3730.4406614811232, 1802.0595116294148, 1461.4575127639082, 1594.169731775011, 4005.306979044437, 2522.5450399263773, 1539.2840948278297, 3861.8185347991803, 264.7599792835253, 157.28778847413452, 153.3073369626756, 149.32688545121667, 111.512596092357, 85.63966126787403, 83.64943551214458, 75.68853248922674, 72.70319385563255, 70.71296809990311, 70.71296809990311, 69.71785522203837, 67.72762946630891, 65.73740371057944, 62.752065076985254, 62.752065076985254, 61.756952199120526, 58.771613565526344, 58.771613565526344, 57.77650068766162, 56.78138780979688, 56.78138780979688, 55.786274931932155, 52.80093629833796, 52.80093629833796, 52.80093629833796, 49.81559766474378, 49.81559766474378, 45.83514615328486, 44.840033275420126, 44.840033275420126, 121.48160564269871, 161.3619594336812, 253.96109939018424, 160.31484890834903, 136.5673749505596, 122.61051510165564, 203.24784842525952, 95.56468342368314, 228.2169047662676, 3730.4406614811232, 266.22475579825243, 90.74162345118653, 158.86744373489842, 484.18532731912165, 139.8307887309293, 318.04387130442893, 851.2829935997243, 176.3975818994345, 587.7495218943192, 920.0779893571485, 181.1327100323193, 1704.230994013829, 472.9256873451171, 358.8029539509097, 185.4665435165091, 920.7979163039074, 635.9097033628491, 2609.2814037738826, 736.8040675234879, 551.7022317391957, 893.6481496928745, 3984.9919399779756, 1003.3580154036865, 1461.4575127639082, 467.4716969163217, 760.1681795570348, 1594.169731775011, 1678.954805407917, 1402.8590688079423, 1644.3262294366086, 868.542099344608, 1966.874500235044, 2778.1757182540327, 1553.2239739598094, 3861.8185347991803], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5506, 1.5506, 1.5506, 1.5505, 1.5505, 1.5504, 1.5504, 1.5504, 1.5504, 1.5504, 1.5504, 1.5504, 1.5503, 1.5503, 1.5503, 1.5502, 1.5501, 1.5501, 1.5501, 1.55, 1.55, 1.55, 1.5499, 1.5499, 1.5498, 1.5498, 1.5497, 1.5497, 1.5497, 1.5497, 1.5404, 1.5377, 1.5178, 1.5243, 1.457, 1.5244, 1.4693, 1.3603, 1.1969, 1.3808, 1.3062, 1.3831, 1.3664, 1.1173, 0.9836, 1.0231, 1.3682, 1.26, 1.1384, 0.839, 1.2259, 1.3303, 0.7849, 0.9109, 1.0296, 1.2974, 0.8645, 0.7415, 0.6716, 0.6136, 1.0826, 0.4934, 0.4787, 0.5078, -0.4179, 0.1833, 0.3907, 0.3799, -0.2988, 0.3393, -0.3613, 0.2675, -0.4282, 1.6835, 1.6832, 1.6832, 1.6831, 1.683, 1.683, 1.6829, 1.6829, 1.6829, 1.6829, 1.6829, 1.6828, 1.6828, 1.6828, 1.6828, 1.6828, 1.6828, 1.6827, 1.6826, 1.6826, 1.6826, 1.6826, 1.6825, 1.6825, 1.6824, 1.6824, 1.6824, 1.6823, 1.6823, 1.6823, 1.6823, 1.6613, 1.6505, 1.6412, 1.565, 1.6469, 1.6322, 1.5578, 1.4716, 1.4921, 1.5502, 1.5517, 1.6088, 1.5422, 1.4274, 1.4081, 1.1742, 1.529, 1.3146, 1.3189, 1.1277, 1.2867, 0.9521, 0.8144, 0.9966, 1.0511, 0.7319, 1.056, 0.4999, 1.0024, 0.5797, 0.4, 1.1112, 0.897, 0.3474, 0.9891, 0.2097, 0.3517, 0.4095, 0.5243, -0.1033, 0.0197, 1.7654, 1.7654, 1.7654, 1.7653, 1.7652, 1.7652, 1.7652, 1.7651, 1.7651, 1.765, 1.765, 1.765, 1.765, 1.7649, 1.7649, 1.7649, 1.7648, 1.7648, 1.7648, 1.7648, 1.7647, 1.7647, 1.7647, 1.7646, 1.7646, 1.7646, 1.7646, 1.7646, 1.7646, 1.7645, 1.7522, 1.7036, 1.7049, 1.6439, 1.648, 1.6663, 1.6974, 1.6505, 1.6584, 1.6565, 1.4921, 1.5399, 1.6527, 1.4398, 1.5216, 1.4418, 1.5011, 1.409, 1.3951, 1.2076, 1.3632, 1.3972, 1.2725, 1.3529, 1.4213, 1.492, 1.4235, 1.3062, 0.9462, 0.6061, 0.8033, 0.6476, 0.3322, 0.899, 0.9411, 0.3334, 0.597, 0.121, 0.2045, -0.173, 0.3578, -0.5923, -0.6945, -0.3361, 1.7914, 1.7911, 1.791, 1.791, 1.791, 1.7909, 1.7908, 1.7908, 1.7908, 1.7908, 1.7907, 1.7906, 1.7905, 1.7905, 1.7905, 1.7904, 1.7903, 1.7903, 1.7903, 1.7902, 1.7902, 1.7902, 1.7902, 1.7901, 1.79, 1.79, 1.79, 1.79, 1.79, 1.7899, 1.7899, 1.7687, 1.7395, 1.7498, 1.7665, 1.7042, 1.7716, 1.6598, 1.6886, 1.6845, 1.657, 1.4232, 1.6554, 1.5664, 1.7411, 1.5255, 1.5844, 1.7037, 1.0816, 1.0061, 1.3443, 1.5366, 1.0856, 1.0008, 1.4982, 0.8952, 1.511, 1.3427, 0.8404, 0.7161, 0.6056, 0.9077, 0.9306, 0.6479, 1.2316, 0.8306, 0.8007, 0.9869, 0.5054, 1.2471, 0.2401, 0.1457, 0.291, 0.41, 0.9859, 0.4816, -0.4288, 0.8077, 0.6542, 1.9032, 1.903, 1.903, 1.9029, 1.9028, 1.9027, 1.9027, 1.9027, 1.9027, 1.9026, 1.9026, 1.9024, 1.9023, 1.9023, 1.9023, 1.9023, 1.9023, 1.9021, 1.9021, 1.9021, 1.902, 1.9019, 1.9019, 1.9019, 1.9019, 1.9018, 1.9018, 1.9018, 1.9018, 1.9017, 1.8153, 1.8513, 1.8189, 1.8151, 1.6962, 1.8493, 1.7734, 1.557, 1.5895, 1.5076, 1.6675, 1.8051, 1.7715, 1.4328, 1.1324, 1.2903, 1.5205, 1.5335, 1.6582, 0.7023, 0.755, 0.8663, 0.9879, 1.0919, 1.0629, 0.695, 0.4276, 0.9237, 1.1401, 0.824, 0.9411, 0.0797, 0.2695, 0.5904, 0.1259, 0.4786, 0.669, 0.7403, 0.3898, 0.7875, 0.5095, -0.3305, 0.1639, 0.2771, 0.1971, -0.616, -0.2722, 0.1861, -0.7193, 2.1587, 2.1586, 2.1586, 2.1586, 2.1585, 2.1583, 2.1583, 2.1582, 2.1582, 2.1582, 2.1582, 2.1582, 2.1582, 2.1581, 2.1581, 2.1581, 2.1581, 2.1581, 2.1581, 2.158, 2.158, 2.158, 2.158, 2.158, 2.158, 2.158, 2.1579, 2.1579, 2.1578, 2.1578, 2.1578, 2.1335, 2.1203, 2.0975, 2.1139, 2.1048, 2.1073, 2.06, 2.1161, 2.032, 1.6656, 1.9814, 2.1005, 2.006, 1.8093, 2.0177, 1.8299, 1.6078, 1.9442, 1.6527, 1.5034, 1.9177, 1.3292, 1.6546, 1.6962, 1.887, 1.3051, 1.4258, 0.8553, 1.3294, 1.4468, 1.2109, 0.4388, 1.005, 0.7219, 1.3, 0.9542, 0.4429, 0.4049, 0.5062, 0.3584, 0.8166, 0.1228, -0.1697, 0.2656, -0.6538], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.1793, -6.3041, -6.4716, -6.5569, -6.6577, -6.8131, -6.831, -6.84, -6.8492, -6.9353, -6.9555, -6.9658, -7.0407, -7.0407, -7.146, -7.25, -7.321, -7.321, -7.4296, -7.4629, -7.48, -7.48, -7.5701, -7.6486, -7.6692, -7.6903, -7.7792, -7.7792, -7.7792, -7.8027, -5.3857, -5.1504, -6.2422, -6.84, -4.8001, -7.223, -6.5299, -5.1985, -4.1406, -5.8268, -5.238, -6.0111, -5.913, -4.1152, -3.7641, -4.1437, -6.3094, -5.8435, -5.4538, -4.6758, -5.9094, -6.2676, -4.6551, -5.0453, -5.603, -6.2676, -5.3121, -5.1354, -5.0141, -5.1255, -5.8951, -5.0713, -5.0791, -5.498, -4.8252, -5.3441, -5.4885, -5.5123, -5.152, -5.4909, -5.1776, -5.5051, -5.281, -4.6562, -6.2902, -6.4294, -6.4896, -6.7568, -6.923, -6.9445, -6.9665, -6.989, -7.0236, -7.0355, -7.1355, -7.1488, -7.1488, -7.1488, -7.2178, -7.2322, -7.3554, -7.3719, -7.3887, -7.4232, -7.4589, -7.4959, -7.515, -7.5744, -7.595, -7.595, -7.705, -7.705, -7.7285, -7.7285, -6.9124, -6.6065, -6.3131, -4.5546, -6.7127, -6.5035, -5.2884, -4.6036, -5.1546, -5.9816, -6.1288, -6.6623, -6.1385, -5.2438, -5.2222, -4.1044, -6.2352, -5.1674, -5.2578, -4.4606, -5.2438, -3.9011, -3.65, -4.6184, -5.0595, -4.1982, -5.2618, -3.9074, -5.1897, -4.3228, -4.3032, -5.5943, -5.2864, -4.5657, -5.4527, -4.6431, -4.8304, -4.9125, -5.1822, -4.9196, -5.0367, -5.8979, -5.9868, -6.0845, -6.2276, -6.4976, -6.4976, -6.5212, -6.713, -6.7725, -6.8915, -6.9148, -6.9266, -6.9879, -7.0669, -7.1233, -7.1233, -7.183, -7.183, -7.1985, -7.2302, -7.332, -7.387, -7.387, -7.4255, -7.4655, -7.4655, -7.4861, -7.4861, -7.4861, -7.5072, -5.9064, -5.2069, -5.8441, -4.9962, -5.3172, -5.7156, -6.367, -5.8979, -5.9915, -6.0104, -4.2859, -5.0549, -6.0743, -4.2306, -5.0242, -4.8698, -5.2799, -4.7321, -4.7468, -3.7054, -4.6855, -5.131, -4.7008, -5.2505, -5.5494, -5.7702, -5.6018, -5.3487, -4.5685, -4.1415, -4.7242, -4.9102, -4.7242, -5.2776, -5.3341, -4.8992, -5.2799, -5.0458, -5.1329, -4.9893, -5.2069, -5.0567, -5.1018, -5.2662, -6.4562, -7.0877, -7.1027, -7.1652, -7.1652, -7.322, -7.3803, -7.3803, -7.4005, -7.4211, -7.4637, -7.5546, -7.6287, -7.6813, -7.6813, -7.7369, -7.7957, -7.7957, -7.7957, -7.8582, -7.8582, -7.8582, -7.891, -7.9249, -7.96, -7.96, -7.9963, -7.9963, -7.9963, -8.034, -8.034, -6.1685, -6.0195, -6.4021, -6.9486, -5.1462, -7.3803, -5.1855, -5.779, -5.7165, -5.775, -3.3982, -6.1745, -5.4964, -7.285, -5.144, -6.3798, -7.1335, -3.6216, -3.4012, -5.181, -6.2049, -4.1592, -3.8155, -6.2362, -3.9575, -6.3226, -5.8943, -4.3417, -4.1371, -3.8588, -4.7631, -4.8939, -4.2546, -5.6506, -4.9329, -5.0102, -5.4176, -4.9347, -5.816, -4.6899, -4.9107, -5.031, -5.2057, -5.6685, -5.4754, -5.3161, -5.633, -5.633, -6.6016, -6.876, -6.9582, -7.0958, -7.1463, -7.2749, -7.2947, -7.3149, -7.3355, -7.378, -7.378, -7.569, -7.6512, -7.6512, -7.6512, -7.6512, -7.6512, -7.7726, -7.7726, -7.7726, -7.8053, -7.8743, -7.8743, -7.8743, -7.8743, -7.9484, -7.9484, -7.9484, -7.9484, -7.9876, -5.6774, -6.685, -6.4193, -6.3785, -5.052, -7.1813, -6.6016, -5.3868, -5.7601, -5.5614, -6.5247, -7.2947, -7.1463, -5.6656, -4.7517, -5.372, -6.237, -6.347, -6.876, -4.2002, -4.567, -4.9435, -5.4017, -5.6462, -5.5937, -4.8933, -4.5025, -5.4671, -5.8838, -5.3291, -5.6159, -4.3847, -4.7869, -5.3633, -5.0563, -5.3868, -5.6423, -5.7015, -5.4606, -5.743, -5.601, -5.2179, -5.4511, -5.5474, -5.5404, -5.4323, -5.5509, -5.5864, -5.5721, -5.3741, -5.895, -5.9206, -5.9469, -6.2391, -6.5032, -6.5267, -6.6268, -6.667, -6.6948, -6.6948, -6.709, -6.738, -6.7678, -6.8144, -6.8144, -6.8304, -6.8799, -6.8799, -6.897, -6.9144, -6.9144, -6.9321, -6.9872, -6.9872, -6.9872, -7.0454, -7.0454, -7.1288, -7.1508, -7.1508, -6.1784, -5.9077, -5.477, -5.9206, -6.09, -6.1954, -5.7372, -6.4357, -5.6493, -3.2218, -5.546, -6.5032, -6.0376, -5.1199, -6.1535, -5.5195, -4.7571, -5.9947, -5.0827, -4.7838, -5.9947, -4.3416, -5.2981, -5.5327, -6.0017, -4.9813, -5.2308, -4.3896, -5.18, -5.3518, -5.1054, -4.3826, -5.1956, -5.1025, -5.6643, -5.5239, -5.2947, -5.2809, -5.3592, -5.3481, -5.5283, -5.4046, -5.3518, -5.498, -5.5066]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 6, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5, 1, 3, 5, 6, 6, 1, 2, 3, 4, 5, 6, 3, 3, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 1, 5, 2, 3, 4, 5, 6, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 5, 6, 1, 1, 6, 1, 3, 4, 5, 6, 1, 6, 1, 2, 3, 4, 5, 6, 3, 2, 5, 2, 4, 5, 6, 4, 1, 1, 4, 6, 4, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 5, 6, 3, 1, 2, 3, 5, 1, 6, 5, 1, 3, 5, 1, 3, 4, 5, 2, 5, 1, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 6, 2, 4, 5, 2, 4, 1, 2, 4, 5, 2, 4, 5, 2, 3, 4, 5, 6, 2, 4, 5, 1, 2, 3, 4, 5, 6, 2, 6, 1, 3, 5, 3, 2, 5, 6, 3, 3, 2, 5, 6, 5, 1, 2, 3, 4, 5, 6, 3, 4, 1, 3, 2, 4, 5, 4, 2, 3, 4, 5, 6, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 5, 5, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 6, 5, 4, 5, 6, 1, 3, 4, 5, 6, 3, 6, 3, 6, 2, 2, 3, 2, 4, 5, 4, 2, 5, 6, 5, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 3, 4, 5, 6, 1, 2, 4, 5, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 3, 3, 2, 5, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 3, 5, 1, 2, 5, 2, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 1, 5, 6, 6, 2, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 2, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 5, 2, 1, 2, 3, 4, 5, 6, 6, 4, 5, 1, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 3, 2, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 1, 2, 5, 6, 4, 3, 5, 6, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 4, 5, 4, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 5, 2, 1, 2, 3, 4, 5, 6, 4, 1, 1, 1, 2, 3, 4, 5, 6, 6, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 6, 5, 5, 1, 2, 3, 4, 5, 6, 4, 2, 1, 2, 3, 4, 5, 6, 2, 4, 4, 1, 2, 4, 5, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 5, 1, 1, 2, 3, 4, 5, 6, 1, 1, 3, 4, 5, 6, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 5, 6, 4, 5, 6, 4, 4, 5, 1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 5, 3, 3, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 6, 6, 6, 6, 2, 1, 2, 3, 4, 5, 6, 5, 2, 5, 2, 2, 2, 1, 2, 3, 4, 5, 6, 1, 1, 1, 2, 3, 4, 5, 6, 1, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 5, 5, 5, 5, 6, 1, 2, 3, 4, 5, 6, 4, 2, 1, 2, 3, 4, 5, 6, 2, 4, 5, 4, 4, 1, 2, 3, 4, 5, 2, 1, 2, 4, 4, 1, 3, 5, 3, 2, 5, 6, 5, 5, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 6, 2, 4, 5, 2, 4, 5, 4, 4, 1, 5, 6, 1, 2, 3, 4, 5, 6, 4, 4, 3, 6, 3, 6, 4, 1, 2, 3, 5, 6, 1, 1, 1, 1, 1, 1, 1, 3, 5, 2, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 5, 5, 4, 5, 5, 2, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 6, 6, 6, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 2, 4, 2, 4, 3, 1, 2, 3, 4, 6, 1, 2, 4, 5, 6, 1, 5, 1, 1, 2, 5, 1, 2, 3, 4, 5, 6, 5, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 3, 1, 3, 3, 1, 1, 2, 3, 4, 5, 6, 3, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 3, 3, 1, 2, 3, 4, 5, 2, 2, 2, 4, 4, 2, 4, 4, 1, 2, 2, 4, 1, 2, 3, 4, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 2, 2, 1, 2, 3, 4, 5, 6, 4, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 3, 1, 1, 5, 5, 3, 2, 4, 5, 4, 1, 2, 3, 4, 5, 6, 3, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 2, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 2, 2, 2, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 2, 2, 3, 1, 3, 4, 5, 1, 3, 6, 5, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 3, 1, 5, 1, 1, 1, 2, 5, 2, 3, 4, 5, 6, 1, 5, 1, 4, 6, 4, 1, 2, 3, 4, 5, 6, 5, 6, 6, 6, 1, 2, 3, 4, 5, 6], \"Freq\": [0.6620714468358873, 0.02359274643513184, 0.21233471791618658, 0.02064365313074036, 0.07962551921856996, 0.9971452152431773, 1.0028656272263186, 0.07328949768355966, 0.16422276332797628, 0.012214916280593277, 0.043430813442109424, 0.270085371093118, 0.4356653473411602, 0.9979952886224197, 0.33147359584141933, 0.038644704933892896, 0.3186827427999196, 0.30072111937994117, 0.010341540756957254, 0.32820918349349454, 0.10517900474888049, 0.3738289927821656, 0.19261697255216667, 0.08164223572607011, 0.7840469879210527, 0.02111437130846641, 0.1140176050657186, 1.0035674979007556, 0.011551566628878329, 0.2629356594573258, 0.5723526227308524, 0.047031378417576056, 0.07178473547945818, 0.03437966258594741, 0.9992160323585303, 1.003232474367925, 0.07794527717187306, 0.9061138471230245, 0.016238599410806888, 0.7017166393865548, 0.01934030282311655, 0.20980024149424256, 0.04078281247483272, 0.022283392383156023, 0.00588617912007895, 0.9956340197948833, 0.9959030389216441, 0.9976291563027231, 0.09247685944443805, 0.6687446965380196, 0.1592657023765322, 0.06507630849793788, 0.014556542690328212, 0.010783616074788024, 0.2264559375705485, 0.7602449332725557, 1.0040591126042309, 0.13994746998045562, 0.23157974199146822, 0.18826266794989863, 0.2510168905998648, 0.1121801148256033, 0.07663790022739236, 0.9212633882108202, 0.006021329334711243, 0.054191964012401186, 0.01806398800413373, 1.0014377790589997, 0.9996168490558152, 1.00411512154523, 0.09799141113299391, 0.02875197093123773, 0.4130895415426809, 0.023470996678561413, 0.4359737633042783, 0.04893544403614516, 0.9460852513654731, 0.27630154777554194, 0.04081727410320506, 0.6310978534418629, 0.0062795806312623175, 0.028258112840680426, 0.01726884673597137, 0.9960100554858883, 0.08411337305266561, 0.9176004333018066, 0.0939903103346289, 0.7662599876433305, 0.10673475919356162, 0.03345417825469842, 1.0034864685839378, 0.9992089786324486, 0.08026311524676895, 0.7979097927472912, 0.1227553527303525, 1.0011774841689043, 0.03004979749540074, 0.13146786404237826, 0.8376381051842957, 0.045192453730509875, 0.6009803496092366, 0.147073687140519, 0.04281390353416725, 0.11337755935899847, 0.0503459791559189, 0.03782851453730803, 0.06147133612312556, 0.12767123656341461, 0.6809132616715446, 0.08984272202610659, 0.1927464063503072, 0.8049996971101067, 0.9892218505076971, 0.10690775699787873, 0.7738845096115102, 0.10850339516202617, 0.009573828984884663, 0.12872701472518094, 0.8653315989859385, 0.9967959576638913, 0.04987360595707285, 0.8900520447723771, 0.05754646841200714, 0.8338118945608711, 0.039289565712292356, 0.043655073013658174, 0.08294463872595054, 0.9732980662950512, 0.021391166292198928, 1.0014377790589997, 0.5898435401159109, 0.08790937376727517, 0.1931879383192136, 0.01878708390994187, 0.04856283954079314, 0.06132387766830083, 0.11367710612932024, 0.7093451422469582, 0.04319730032914169, 0.06820626367759214, 0.06593272155500574, 0.8252477310909566, 0.10432376977942283, 0.07006820358319443, 0.9670412138282872, 0.03453718620815312, 0.059520121018355555, 0.6939175084579013, 0.08129577504946126, 0.16549497063640325, 0.9245794226354228, 0.015804776455306372, 0.05531671759357231, 0.4998214829048273, 0.0342517259858638, 0.128126826836009, 0.3120712812045368, 0.02664023132233851, 0.07910203982287582, 0.7770612147306037, 0.14424489614759708, 0.21454836298594931, 0.4555478940112622, 0.03722756983317842, 0.11560140106092245, 0.09208925169259925, 0.08523154146017164, 1.00390522782623, 1.004190876910395, 0.08043123520545345, 0.8847435872599879, 0.034028599509999535, 0.9960100554858883, 0.038645697945727184, 0.1766660477518957, 0.7839555868990372, 1.0030346390682465, 0.9892218505076971, 0.06531374425296595, 0.9143924195415233, 0.018142706736934987, 0.996540722185999, 0.4649301556123612, 0.026088265244781788, 0.1514051107956086, 0.2762560944670643, 0.026088265244781788, 0.0549717017657902, 0.02463859230093838, 0.973224395887066, 0.012850072333738508, 0.9851722122532857, 0.14286572961389124, 0.7551474279591394, 0.10204694972420802, 0.9947893519267437, 0.08156573856542364, 0.010875431808723152, 0.179444624843932, 0.23744692782378882, 0.49120700336066236, 0.016041683499879115, 0.03208336699975823, 0.689792390494802, 0.2620141638313589, 0.08187425096361987, 0.16736060123445826, 0.03250889376496671, 0.1420759060839286, 0.39973898999884994, 0.1757888329513015, 0.1135302101653433, 0.1680247110447081, 0.013623625219841196, 0.1135302101653433, 0.46547386167790755, 0.12488323118187764, 1.000906559658768, 0.9977085812055516, 0.9976832079869428, 0.8270786644924308, 0.008555986184404457, 0.052761914803827484, 0.014259976974007429, 0.07842987335704087, 0.0199639677636104, 0.999028420984231, 0.845103054378866, 0.036171726062908666, 0.05261341972786715, 0.06247843592684224, 0.006576677465983394, 0.9987822505946133, 0.9966323301378297, 0.11690744158681411, 0.3811372689131094, 0.019009340095416927, 0.08364109641983448, 0.3393167207031922, 0.05892895429579248, 0.9967959576638913, 0.0719666282514555, 0.7813519638729454, 0.143933256502911, 0.503066130314416, 0.4203569986379646, 0.016541826335290274, 0.018487923551206778, 0.040868041534246564, 0.9002118769955088, 0.10191077852779344, 0.8983105174220153, 0.10365121354869407, 1.0015913774792589, 0.9935832642563407, 1.0008594577651138, 0.5424998370558924, 0.013647794642915533, 0.4435533258947548, 1.005659240921184, 0.27152160290867206, 0.6237658445199223, 0.10518404437002613, 0.9966323301378297, 0.3594708395177929, 0.039096526352388354, 0.01194616082989644, 0.16398820775585113, 0.42571773139267316, 0.2406180548001813, 0.053904694094845815, 0.20155668226768433, 0.2695234704742291, 0.08437256467019344, 0.14921444307413842, 0.2979691140484686, 0.2284659257831214, 0.06330983485556377, 0.1816717000203134, 0.11216851175496623, 0.11629741402815517, 1.0068660764253847, 0.018870603698199616, 0.06919221356006526, 0.43087878444222455, 0.4812003943040902, 0.06462987619368499, 0.8820077221726421, 0.02851318067368455, 0.02471142325052661, 0.021908015083595328, 0.14605343389063552, 0.029210686778127104, 0.7886885430094318, 0.014605343389063552, 0.09117200714609824, 0.10809052393609585, 0.6908394355915691, 0.06579423196110182, 0.03477695117943953, 0.01033909359388743, 0.06081513288163334, 0.9406073885692623, 0.9950549158286489, 0.9947974935035944, 0.8568424679065225, 0.1428070779844204, 0.15576552317329284, 0.5334128707229309, 0.09861414416726454, 0.12214706493445267, 0.08964922197024049, 0.069723935825047, 0.1910145325207017, 0.045756332885187094, 0.13073237967196313, 0.462647365839114, 0.10022815774850508, 0.5671462625833839, 0.06130680931337031, 0.3135272187318748, 0.006993172165023229, 0.010722863986368951, 0.04032729281830062, 0.7761292701078399, 0.2067077955563594, 0.01560058834387618, 0.18366185164088364, 0.31380634253872836, 0.027366832198144914, 0.15629501944273871, 0.1532542603096115, 0.16541729684212036, 0.9913078025573756, 0.25471884451200616, 0.08948206912996623, 0.3818240563443446, 0.0645694476108279, 0.07829681048872046, 0.13066415776364387, 0.06427311330521109, 0.9319601429255608, 0.9973561425861863, 0.026269792689860384, 0.8756597563286794, 0.09632257319615474, 0.9978391213354094, 0.08421678409180475, 0.5647478462626907, 0.047887975267888974, 0.07430892713982772, 0.17008487767560568, 0.059447141711862175, 0.9953225221906374, 0.19634044842357753, 0.15368501553922204, 0.19320401953502198, 0.09472015243437766, 0.1812855897585109, 0.18003101820308867, 0.01181283278109778, 0.04725133112439112, 0.9410890115607898, 1.0040591126042309, 0.32539311779725444, 0.6721624258147665, 0.0023751322466952877, 0.043053704487516394, 0.040901019263140576, 0.7082334388196447, 0.055969815833771315, 0.09687083509691188, 0.0538171306093955, 0.27740168396124454, 0.12473330988421301, 0.23062669275466469, 0.12993053112938854, 0.1793041329585562, 0.05846873900822485, 1.0040215571671023, 0.09052656186198227, 0.8750900979991618, 0.030175520620660753, 0.017169276331358375, 0.6920539075101376, 0.011886422075555798, 0.07395995958123608, 0.13867492421481764, 0.06735639176148286, 0.9880544174010915, 0.9986147513693472, 0.2437570938279586, 0.025771675118374478, 0.4381184770123661, 0.1728849872524288, 0.0912746827109096, 0.02791931471157235, 1.0035966689440419, 0.44487732031204547, 0.05191437125197289, 0.3107651945777822, 0.14492761974509097, 0.020909955087600192, 0.025957185625986444, 0.6481127815043071, 0.047321318743013076, 0.23887801701473, 0.03747848444446635, 0.006435699349049778, 0.021957091896758065, 1.0037700790102904, 0.06108426973281056, 0.05051199227905488, 0.08810231211463061, 0.08810231211463061, 0.13509021190910028, 0.5767764699771151, 1.0038500677534585, 0.03940937559817841, 0.867006263159925, 0.05692465364181326, 0.035030556087269696, 0.9975545279811994, 0.05408956209098751, 0.07828752407906087, 0.027044781045493755, 0.2405562103520234, 0.38147375369433295, 0.21920506742137044, 1.0038683428327828, 0.05067286516499881, 0.9458934830799778, 0.05125675149378945, 0.9445887060998341, 0.34223055829761617, 0.05712577466223609, 0.3270319576994066, 0.09905294872626258, 0.08175798942485164, 0.0927638726166586, 0.11554120928450407, 0.18950028006277178, 0.23838309937544658, 0.19267448910904936, 0.19521385634607144, 0.06856291539959582, 0.994257761359548, 0.9979750660903848, 0.887899156964793, 0.05239156516251884, 0.05790646675857346, 0.001838300532018205, 0.9999791266030181, 0.017233601778595058, 0.0930614496044133, 0.010340161067157034, 0.04653072480220665, 0.8134260039496867, 0.020680322134314068, 0.01753594211421416, 0.026303913171321244, 0.8767971057107081, 0.07891173951396373, 0.9913078025573756, 0.7675257770801145, 0.03027363853912285, 0.20123065617181657, 0.8885486786563513, 0.01498756807372159, 0.09634865190249593, 0.39140437895471736, 0.11748413942942561, 0.21549229852563093, 0.1300492880315032, 0.10366247596714023, 0.04209324781695997, 0.050920067068757825, 0.2798533767356121, 0.09066255843949564, 0.22313752967528835, 0.3171119623956788, 0.038500538515402256, 0.03538762817068336, 0.9023845183524257, 0.06369773070723006, 1.0007202505843926, 0.12019854371228464, 0.0886710568369313, 0.1379327550796709, 0.5714356996157794, 0.05320263410215877, 0.027586551015934182, 1.0034473809376727, 0.06287070795558315, 0.0817319203422581, 0.023052592917047157, 0.07334915928151368, 0.7062476193677174, 0.05239225662965263, 0.9967959576638913, 1.0011464163650907, 0.03707468873305508, 0.26384220234300365, 0.04643334802489423, 0.3862246700055156, 0.16917576412170765, 0.09754602569570803, 1.008165537608962, 1.0011396334693576, 1.0011396334693576, 0.08467831982231323, 0.039241172600584184, 0.02065324873714957, 0.06195974621144871, 0.08880896956974314, 0.7042757819368003, 0.9954032449219238, 0.10700744973506242, 0.03147277933384189, 0.8560595978804993, 0.7474290899700167, 0.0860526912794427, 0.07130080134582396, 0.02458648322269792, 0.05654891141220521, 0.01475188993361875, 0.9978976085502141, 0.8089555632699126, 0.037809879587615475, 0.11430893828813982, 0.03868917911290886, 0.30973239480849457, 0.15816122288093337, 0.1200853729281161, 0.16035790653205745, 0.15303562769497722, 0.09811853641687535, 0.05510150479828803, 0.9477458825305541, 0.9972216407901721, 0.9972216407901721, 0.3624660393661336, 0.06119556508778879, 0.03608969223126006, 0.44719836025691806, 0.010983819374731321, 0.08316320383725144, 1.0025050190925986, 0.9966082561108272, 0.07402742617618609, 0.2771568998102389, 0.08294639318536513, 0.4916580563809949, 0.03768263561378147, 0.036790738912863566, 1.0000814929753432, 0.9977286462230428, 1.0007202505843926, 0.03696281083555505, 0.13861054063333142, 0.7438765680655453, 0.07854597302555447, 0.05497692490749931, 0.20722071695903585, 0.12475532959778689, 0.010572485559134483, 0.6047461739824924, 0.012653261745590883, 0.39225111411331737, 0.004744973154596582, 0.025306523491181766, 0.5409269396240103, 0.022143208054784046, 0.08775540823554077, 0.9141188357868831, 0.99881773604313, 0.7233085071633952, 0.032993019624996976, 0.05583434090384103, 0.10151698346152915, 0.04314471797114989, 0.04568264255768811, 0.9935685949960861, 0.20468620400949672, 0.6915074459780295, 0.047022506326506, 0.023511253163253, 0.033192357406945414, 0.051042151260812386, 0.017014050420270797, 0.18034893445487044, 0.149723643698383, 0.6022973848775861, 0.1414006189485776, 0.15921487015469762, 0.006680344202295005, 0.16478182365661012, 0.2905949727998327, 0.23715221918147267, 0.1527282309668119, 0.7607594900988367, 0.046106635763565856, 0.04034330629312013, 0.9162522154669979, 0.022906305386674947, 0.061083481031133195, 0.9991391162184394, 0.9920695235461285, 0.9966323301378297, 0.006574203977528814, 0.08076879172392543, 0.6996831376084237, 0.15684172346390168, 0.04883694383307119, 0.007513375974318644, 0.08464103634227693, 0.7239036002957896, 0.13587113728628666, 0.047889007404183004, 0.007795884926262349, 0.09623474297972434, 0.07497357883304105, 0.31891746220024925, 0.015666120950187684, 0.10630582073341642, 0.3871769891974956, 0.9970018879803064, 0.9970018879803064, 1.0018532331204963, 1.0012353019946552, 0.5941605057162972, 0.09979440018490779, 0.07830022168354303, 0.10747089250682378, 0.0491295508602623, 0.07062372936162706, 1.0038311406941693, 0.09069544330433985, 0.08770548363496601, 0.2621197976817734, 0.04684270148685685, 0.19733733817867352, 0.3149424185073779, 1.0039946251996303, 1.003951024125032, 1.0037700790102904, 1.0037700790102904, 1.0009927935504939, 0.1354006337889342, 0.5033130116662431, 0.04661333294373145, 0.12763174496497895, 0.17535491916927542, 0.011653333235932862, 0.9977329407096336, 0.8753062148674956, 0.12450476332166964, 1.0027376119697107, 0.9892373876757292, 0.9880544174010915, 0.35269057185703545, 0.2796918255889514, 0.01722442327673894, 0.12795285862720357, 0.16404212644513277, 0.059055165520247796, 1.0018422364019341, 0.9974593598725849, 0.03404055774188322, 0.5738265447917457, 0.05349230502295935, 0.012427505207354191, 0.2988004512898638, 0.028096968294887736, 1.0014377790589997, 0.5277195939560491, 0.24902744152659714, 0.1975045225900598, 0.02576145946826867, 0.3078266182311873, 0.12497234500838801, 0.16575279443217777, 0.032887459212733686, 0.06972141353099541, 0.29993362802013124, 0.9976291563027231, 0.9972216407901721, 0.9972216407901721, 0.04366407758024869, 0.9543719813968643, 0.18177164454351233, 0.09979619700428129, 0.18034598458630832, 0.10621166681169936, 0.24022370278887709, 0.19175126424394046, 1.0068660764253847, 0.994124455753383, 0.01633176652594032, 0.41935761789188697, 0.09465400169335304, 0.3053864729958088, 0.1613859509391307, 0.0029853766767847898, 0.037883607055081, 0.947090176377025, 0.014206352645655375, 1.0034287402095678, 1.0068660764253847, 0.15725816330468984, 0.4813602428667442, 0.010103901760927354, 0.34120099023439293, 0.010362976165053696, 0.9892373876757292, 0.017074649217802433, 0.10927775499393556, 0.8742220399514845, 1.0034473809376727, 0.19365958467397248, 0.7982914177400392, 0.008869904641555992, 0.9983745602439306, 0.02060390957762169, 0.9477798405705977, 0.030905864366432535, 0.9979482345870316, 0.9972216407901721, 0.15655804039929072, 0.6620168565455722, 0.09691688215194187, 0.04323983972932791, 0.04174881077314419, 0.04667512577228149, 0.017565907548708085, 0.004516947655382079, 0.6915948743462783, 0.060476910274837835, 0.1791722569968225, 0.9790521544156716, 0.02263704403273229, 0.047475768938309626, 0.8754531792224295, 0.07596123030129541, 0.03507211237849632, 0.9609758791707992, 0.007014422475699264, 0.9964626237557261, 0.9916808996310768, 0.018591742505661518, 0.018591742505661518, 0.9605733627925117, 0.1314538169412884, 0.006131956475395378, 0.016862880307337288, 0.49362249626932786, 0.08048192873956433, 0.27172232131595764, 0.990272893269122, 1.004535468365547, 0.04185646681071627, 0.9626987366464741, 0.9950549158286489, 1.0038500677534585, 0.9928973972504912, 0.9101189236797417, 0.020037380706738227, 0.0316379695369551, 0.03269256852152027, 0.005272994922825849, 0.9981438077068174, 0.9994162153592555, 0.9997500656758539, 1.0019471301962464, 1.0034851020526594, 0.9981371113935942, 0.05044746119341272, 0.8942959029741345, 0.05503359402917751, 0.06375381515619832, 0.5808680936453625, 0.23848649373244557, 0.11806262065962651, 1.008165537608962, 0.13636076411059794, 0.26259572890605243, 0.08910703397326201, 0.37127930822192506, 0.12826012465848322, 0.012150959178172093, 0.16292722637395596, 0.24790220219830367, 0.12149314725299301, 0.18539994725312228, 0.21981130109934574, 0.06250225494518138, 1.0042076151025265, 0.9969378116533533, 0.9967959576638913, 0.09049391459938605, 0.9049391459938606, 0.9970018879803064, 1.0044354426441666, 0.4909692670816654, 0.4407679105293479, 0.04568323446260895, 0.022590610448542887, 0.10236772201928053, 0.054724882840495884, 0.1590240713129704, 0.3824303577324065, 0.15065438334912984, 0.15065438334912984, 0.9981703063096863, 1.003951024125032, 1.0039355536862606, 1.0037016987429768, 1.0037016987429768, 0.12925090031680786, 0.15572397628531068, 0.2304714849022598, 0.1253578009096751, 0.2686238590921609, 0.08954128636405363, 0.25456086600532996, 0.021391669412212603, 0.1240716825908331, 0.07914917682518663, 0.09626251235495671, 0.42355505436180957, 0.12262459213581112, 0.1572110155587322, 0.7200264512589934, 0.055448200860615894, 0.1265665454427102, 0.18322014197420905, 0.08317230129092384, 0.37487805364459875, 0.17598776794891133, 0.18951906401891985, 0.044592720945628196, 0.019509315413712338, 0.016722270354610574, 0.09754657706856168, 0.6298721833569983, 0.9978109404061504, 0.9524660229240117, 0.04831349391643538, 0.9662643214880063, 0.031169816822193754, 0.9924075013848338, 0.8430845251008288, 0.06275386432789341, 0.035469575489678884, 0.035469575489678884, 0.02182743107057162, 0.7827824413095896, 0.10680693992734078, 0.018269608145466184, 0.06886236916368023, 0.02248567156365069, 0.9903840970119147, 0.010295052983491836, 0.999616105736758, 0.9974002409083262, 0.6723066767880599, 0.32714304272573635, 0.3472386300182605, 0.16675897733399744, 0.24486128317397093, 0.055938137966467495, 0.11187627593293499, 0.07335284129565077, 0.9966323301378297, 0.04381796348627933, 0.0394361671376514, 0.035054370789023466, 0.8807410660742145, 0.9929257047824875, 0.4151392269493863, 0.09887103539971037, 0.13401194557189658, 0.09470177487080692, 0.0839808192250552, 0.17332211627298624, 0.306495567497324, 0.030967168736776258, 0.610609045091819, 0.044465678186140264, 0.007940299676096477, 0.9913254534079986, 0.057573789076701264, 0.9403718882527874, 0.9968606091905079, 0.9935685949960861, 0.3136655829682439, 0.16189191379006135, 0.1691192313699748, 0.07877776162105664, 0.15683279148412194, 0.11925074006857198, 0.998932300074237, 0.07390677832377132, 0.004347457548457136, 0.3206249941987138, 0.08151482903357131, 0.5195211770406278, 0.6265922753430976, 0.23199091163221952, 0.026017672332585368, 0.054203484026219516, 0.06287604147041463, 0.9973008603900577, 0.9913254534079986, 0.989676873170148, 0.13965314400237752, 0.30617542022373984, 0.08541730304183423, 0.4557800946775932, 0.012936989586918583, 0.9981117436142353, 0.9971655305682927, 0.05044279019470506, 0.9584130136993961, 1.005659240921184, 0.12708580729878494, 0.8743503542156404, 1.0016039739196059, 0.04029713371057649, 0.9613744756666105, 1.0036679652647167, 1.0025050190925986, 0.16015753641454544, 0.06181518949333333, 0.13767928568969698, 0.6378203643175757, 0.04716745030191673, 0.11005738403780571, 0.034589463554738936, 0.7295232313363121, 0.07861241716986121, 0.13827682351930312, 0.22916664572019338, 0.09218454901286875, 0.40809789113562117, 0.07250470147079564, 0.06007532407580211, 0.9966082561108272, 0.9966082561108272, 0.06481982167606008, 0.11998562735781335, 0.12550220792598868, 0.6399233459083379, 0.04275349940335878, 0.00827487085226299, 1.0025050190925986, 0.9186819763130707, 0.08351654330118824, 1.0040824361163119, 0.047029574498933205, 0.032006238200662875, 0.7609646429341276, 0.030699861131248063, 0.054214648380714665, 0.07446349295664424, 0.9887474901309816, 0.989676873170148, 1.0005537863843192, 0.9959030389216441, 0.9983301203510099, 0.9981414600914852, 0.9892218505076971, 0.02979438148891358, 0.8971419314995089, 0.0728307103062332, 1.0007202505843926, 0.1014724725381692, 0.04516404847074583, 0.7220382294478976, 0.03753894937828225, 0.0633469770758513, 0.030500396369854327, 0.9905334499984655, 0.9992220580020382, 0.9977563460595126, 0.14780390194741977, 0.16752773345729502, 0.1438092018947868, 0.4533984559738417, 0.08039333855923846, 0.006990725092107691, 0.040697640568518434, 0.14534871631613727, 0.8139528113703687, 0.0073327580089691926, 0.0024442526696563977, 0.44729823854712075, 0.2945324466935959, 0.24809164597012434, 1.0038859990498479, 0.07944347205744733, 0.24178448017483967, 0.008059482672494655, 0.13701120543240913, 0.27171970152981983, 0.26135750952232667, 0.031626855125947, 0.506029682015152, 0.10278727915932774, 0.17493604241539432, 0.18481943464225276, 0.9944138408791635, 1.0027169704926722, 1.0014454177079313, 0.02460050641981902, 0.009840202567927609, 0.05904121540756565, 0.9052986362493399, 0.02182093359707554, 0.386062671332875, 0.1222531792554104, 0.2117749581152075, 0.22856029165141944, 0.02937433368837092, 0.01684937111907771, 0.7597534613693222, 0.10262798772529151, 0.12100911985519447, 1.0033209910341996, 0.9944138408791635, 1.0006044523304118, 0.8020893280077261, 0.12494858878512317, 0.060458994573446695, 0.012091798914689339, 0.97164125774888, 0.026026105118273572, 1.0035674979007556, 0.9980740987066197, 0.15642838813820903, 0.1490115249075181, 0.15507986755081068, 0.09507070141158394, 0.35466091448576703, 0.08967661906199052, 0.8324951255660018, 0.06742037988738747, 0.07328302161672551, 0.029313208646690205, 1.003232474367925, 0.9866567156790965, 0.0129823252063039, 0.9968421849985951, 0.9951224169762911, 0.967040389732052, 0.0237019703365699, 0.00948078813462796, 0.11153249312854428, 0.031475427508668934, 0.42286552609472616, 0.1963792977171301, 0.23743420316322003, 0.9797493096920269, 0.025448034017974727, 0.9926410202774667, 0.9724973552168936, 0.01906857559248811, 0.9977286462230428, 0.0734497676988144, 0.03860133776871997, 0.0613868496460894, 0.10856626247452492, 0.10695787340082825, 0.6106517183135006, 0.02469509671137859, 0.971340470647558, 1.0038859990498479, 1.0040469514884394, 0.01001974443902537, 0.5310464552683446, 0.029148347458982895, 0.15302882415966018, 0.2477609534013546, 0.029148347458982895], \"Term\": [\"accomplish\", \"accomplish\", \"accomplish\", \"accomplish\", \"accomplish\", \"accomplish step\", \"activity helpful\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"anne\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer question\", \"answer question\", \"answer question\", \"answer question\", \"art\", \"art\", \"art\", \"art\", \"art score\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask extra\", \"ask extra credit\", \"ask teacher\", \"ask teacher\", \"ask teacher\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment due\", \"assignment none\", \"atom\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"author\", \"author\", \"author\", \"author craft\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"block\", \"block\", \"block\", \"block\", \"block finish\", \"block reflection\", \"body paragraph\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book club\", \"book club\", \"bring\", \"bring\", \"bring\", \"bring\", \"bring\", \"bring\", \"cabrera\", \"calculation\", \"calculation\", \"carefully\", \"carefully\", \"carefully\", \"carefully\", \"carefully look\", \"change finish\", \"chapter\", \"chapter\", \"chapter\", \"chapter note\", \"character\", \"character\", \"character\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"chose\", \"chose\", \"chose\", \"chose\", \"chose\", \"claim\", \"claim\", \"classmate adult\", \"classwork\", \"classwork\", \"classwork\", \"classwork\", \"club\", \"club\", \"coaster\", \"cod\", \"cod\", \"cod\", \"college\", \"college\", \"college\", \"college\", \"collins\", \"collins\", \"color apply\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"concept\", \"concept\", \"concept\", \"concept quiz\", \"concept quiz\", \"confident\", \"confident\", \"confident\", \"confident\", \"confident answer\", \"confident answer\", \"confident answer\", \"confuse\", \"confuse\", \"confuse\", \"confuse\", \"confuse\", \"consummative\", \"consummative\", \"consummative\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue\", \"continue ask\", \"craft\", \"credit\", \"credit\", \"credit\", \"credit packet\", \"criterion\", \"criterion\", \"criterion\", \"crosby\", \"current increase\", \"data\", \"data\", \"data\", \"data table\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day test\", \"day test\", \"deduction\", \"deduction\", \"definition\", \"definition\", \"definition\", \"deposition\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"diagram\", \"diagram\", \"diagram\", \"diagram\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"dish\", \"double number\", \"double number line\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due\", \"due date\", \"early\", \"early\", \"early\", \"early\", \"early\", \"earn pillar\", \"earthquake\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"effective test\", \"element\", \"element\", \"element\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english language\", \"english language\", \"english language art\", \"english language art\", \"enough answer\", \"enough answer question\", \"enrichment\", \"equation\", \"equation\", \"equation\", \"erosion deposition\", \"error\", \"error\", \"error\", \"eruption\", \"essay\", \"essay\", \"essay\", \"essay\", \"essay\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything\", \"everything remember\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"exam\", \"exam\", \"exam\", \"exam\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"extra\", \"extra\", \"extra\", \"extra\", \"extra\", \"extra\", \"extra credit\", \"extra credit\", \"extra credit packet\", \"extra packet\", \"factor\", \"factor\", \"felt\", \"felt\", \"felt\", \"felt\", \"felt\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish homework\", \"finish homework\", \"finish homework\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first read\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"folder\", \"folder\", \"found effective\", \"function\", \"function\", \"function\", \"function notation\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future\", \"future continue\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"grammar\", \"grammar\", \"grammar\", \"grammar punctuation\", \"graph\", \"graph\", \"graph\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"heading\", \"heat\", \"heat\", \"heat\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"helpful ask\", \"helpful study\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"highlight key\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"homework\", \"homework\", \"homework\", \"homework\", \"homework\", \"homework\", \"hook\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"increase score point\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent practice\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"ink\", \"interpret\", \"interpret\", \"introduction\", \"introduction\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"keep\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know achieve\", \"know reach\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge check\", \"lab\", \"lab\", \"lab\", \"lab\", \"lab\", \"lab\", \"label\", \"label\", \"label\", \"label\", \"landforms\", \"language\", \"language\", \"language\", \"language art\", \"language art\", \"language art\", \"last\", \"last\", \"last\", \"last\", \"last\", \"last\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn target\", \"learn target\", \"learn target\", \"learn target assessment\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let study\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear equation\", \"little mistake\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look learn target\", \"lordly\", \"lordly wise\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main dish\", \"main idea\", \"main idea\", \"main idea\", \"manage\", \"manage\", \"manage\", \"manage\", \"manage\", \"manage\", \"mass\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"narrative\", \"narrative\", \"nett\", \"nett square\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night test\", \"notation\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note classwork\", \"note put\", \"note reading\", \"note take\", \"note take\", \"note take\", \"note take\", \"notebook\", \"notebook\", \"notebook\", \"notebook\", \"notebook\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number line\", \"number line\", \"object stroke\", \"outside\", \"outside\", \"outside\", \"outside\", \"outside\", \"outside\", \"outside classroom\", \"packet\", \"packet\", \"packet\", \"packet\", \"packet\", \"paragraph\", \"paragraph\", \"paragraph\", \"paragraph\", \"paragraph\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"participate\", \"participate\", \"participate\", \"participate\", \"passage\", \"passage\", \"passage\", \"passage answer\", \"passage read\", \"path\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay attention\", \"pay attention\", \"pay attention\", \"pay attention\", \"pay attention\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"periodic\", \"periodic table\", \"pillar\", \"pillar point\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"pod cast\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point first\", \"point first semester\", \"point third\", \"point third semester\", \"polynomial\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"precise\", \"predict\", \"predict\", \"predict topic\", \"predict topic assessment\", \"predict topic knowledge\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"preview\", \"preview note\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"procrastination\", \"project\", \"project\", \"project\", \"project\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"pun\", \"pun nett\", \"pun nett square\", \"punctuation\", \"punctuation\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put\", \"put away write\", \"quadratic\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question carefully\", \"question carefully\", \"question carefully\", \"question read\", \"question read passage\", \"quiz\", \"quiz\", \"quiz\", \"quiz\", \"quiz\", \"quiz exam\", \"quiz let\", \"quiz let\", \"quiz let\", \"quiz let study\", \"raise\", \"raise\", \"raise\", \"raise hand\", \"rate\", \"rate\", \"rate\", \"ratio\", \"ratio rate\", \"reach\", \"reach\", \"reach\", \"reach\", \"reach\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read passage\", \"read passage\", \"read question\", \"read question\", \"read question\", \"read question carefully\", \"read question carefully\", \"read question carefully\", \"read question first\", \"read question read\", \"reader\", \"reader\", \"reader\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading passage\", \"reading question carefully\", \"reading write\", \"reading write\", \"receive deduction\", \"red ink\", \"refer note\", \"reflection\", \"reflection\", \"reflection\", \"reflection\", \"reflection\", \"reflection accomplish\", \"reflection accomplish step\", \"reflection change\", \"reflection change yes\", \"reflection complete\", \"reflection finish\", \"report\", \"report\", \"report\", \"reread\", \"reread\", \"reread\", \"reread\", \"reread story\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"risk\", \"roller\", \"roller coaster\", \"sba\", \"sba\", \"sba test\", \"schedule realistic\", \"school\", \"school\", \"school\", \"school\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score point\", \"score point first\", \"score point first semester\", \"score point third\", \"score point third semester\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"semester\", \"semester\", \"semester\", \"semester\", \"semester\", \"semester\", \"sentence\", \"sentence\", \"sentence\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"side\", \"side\", \"side\", \"side\", \"side\", \"side\", \"side dish\", \"silly\", \"silly\", \"silly mistake\", \"silly mistake\", \"sketch day\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small win\", \"small win\", \"small win outside\", \"small win outside classroom\", \"solve\", \"solve\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"specific heat\", \"spell\", \"spell\", \"spell\", \"spell\", \"square root\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay lunch\", \"stay school\", \"stay school\", \"stay school finish\", \"stay top\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stop talk\", \"story\", \"story\", \"story\", \"story\", \"story\", \"stress\", \"stress\", \"stress\", \"stress\", \"stress\", \"stroke\", \"stuck back\", \"stuck back note\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study activity\", \"study activity helpful\", \"study day\", \"study day\", \"study learn target\", \"study note\", \"study note\", \"study quiz let\", \"study schedule\", \"study schedule\", \"study schedule realistic\", \"study session\", \"study test\", \"study test\", \"study test\", \"study test\", \"table\", \"table\", \"table\", \"table\", \"table\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take knowledge\", \"take knowledge check\", \"take note\", \"take note\", \"take note\", \"take note\", \"take note\", \"take note\", \"take note study\", \"take read\", \"take read\", \"take risk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk much\", \"talk talk\", \"tam\", \"tam lesson\", \"tape\", \"tape diagram\", \"tardy\", \"target\", \"target\", \"target\", \"target assessment\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher ask\", \"teacher talk\", \"technology\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test study\", \"test study\", \"test study\", \"text\", \"text\", \"text\", \"text\", \"text\", \"third semester\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic assessment\", \"topic knowledge\", \"topic knowledge check\", \"transition\", \"transition\", \"transition\", \"transition\", \"understand\", \"understand\", \"understand\", \"understand\", \"understand\", \"understand\", \"understood\", \"understood\", \"understood\", \"understood\", \"understood instruction\", \"understood question\", \"unfinished\", \"upcoming\", \"upcoming\", \"upcoming\", \"upcoming\", \"upcoming assignment\", \"upcoming assignment\", \"vocab com\", \"volcano\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"weekend\", \"weekend\", \"weekend\", \"weekend\", \"weekly report\", \"win\", \"win\", \"win outside\", \"win outside classroom\", \"wise\", \"wise\", \"wise\", \"word\", \"word\", \"word\", \"word\", \"word\", \"wordy\", \"wordy\", \"wordy wise\", \"workbook\", \"workbook\", \"workbook page\", \"write\", \"write\", \"write\", \"write\", \"write\", \"write\", \"write reading\", \"write reading\", \"write write\", \"writer\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"wrong\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 3, 6, 2, 5, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el69151082186407822958678\", ldavis_el69151082186407822958678_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el69151082186407822958678\", ldavis_el69151082186407822958678_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el69151082186407822958678\", ldavis_el69151082186407822958678_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
       "topic                                                    \n",
       "3     -158.143997 -155.486206       1        1  21.206582\n",
       "2      279.716858  -79.204399       2        1  18.572105\n",
       "5      151.260803  163.590973       3        1  17.108216\n",
       "1       88.502220 -276.410431       4        1  16.666463\n",
       "4       48.392509  -46.213669       5        1  14.901539\n",
       "0     -119.351021  116.455246       6        1  11.545094, topic_info=     Category         Freq      Term        Total  loglift  logprob\n",
       "7167  Default  3730.000000     write  3730.000000  30.0000  30.0000\n",
       "4975  Default  3984.000000      read  3984.000000  29.0000  29.0000\n",
       "6126  Default  6029.000000     study  6029.000000  28.0000  28.0000\n",
       "4701  Default  5694.000000  question  5694.000000  27.0000  27.0000\n",
       "4835  Default  3859.000000      quiz  3859.000000  26.0000  26.0000\n",
       "...       ...          ...       ...          ...      ...      ...\n",
       "6664   Topic6   226.895687   thought   868.542099   0.8166  -5.5283\n",
       "2315   Topic6   256.749074     focus  1966.874500   0.1228  -5.4046\n",
       "3502   Topic6   270.680654      look  2778.175718  -0.1697  -5.3518\n",
       "5509   Topic6   233.861477     score  1553.223974   0.2656  -5.4980\n",
       "6349   Topic6   231.871252      take  3861.818535  -0.6538  -5.5066\n",
       "\n",
       "[483 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "29        1  0.662071  accomplish\n",
       "29        2  0.023593  accomplish\n",
       "29        3  0.212335  accomplish\n",
       "29        4  0.020644  accomplish\n",
       "29        5  0.079626  accomplish\n",
       "...     ...       ...         ...\n",
       "7272      2  0.531046       wrong\n",
       "7272      3  0.029148       wrong\n",
       "7272      4  0.153029       wrong\n",
       "7272      5  0.247761       wrong\n",
       "7272      6  0.029148       wrong\n",
       "\n",
       "[1084 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 3, 6, 2, 5, 1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(model, X_ngrams, token_vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(panel, 'GuidedLDA_6topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Oct/2019 20:48:50] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2019 20:48:50] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2019 20:48:50] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2019 20:48:50] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.show(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shahrzad/Desktop/Insight_Project_SHV/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_cleaning_and_splitting.ipynb\r\n",
      "Gensim LDA_BOW_TFIDF_with visualization.ipynb\r\n",
      "Guided LDA_6topics-4-grams.ipynb\r\n",
      "LDA_GridSearch.ipynb\r\n",
      "POS_tagger.ipynb\r\n",
      "basic_EDA.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
